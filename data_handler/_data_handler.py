import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from data_handler.short_squeeze_scanner2 import ShortSqueezeScanner
from utils._polygon.polygon_premarket_fetcher import PolygonController
from polygon import RESTClient
from polygon.rest.models import TickerSnapshot

from get_sec_filings.get_sec_filings_6_demo_cache import SECFinancialAnalyzer
from utils._database._mongodb.mongo_handler import MongoHandler

from zoneinfo import ZoneInfo
from utils.logger.shared_logger import logger
from datetime import datetime, timedelta
import time
import json

from openai import OpenAI
from api_polygon.api_chart import ChartAnalyzer
from utils._news.api_news_fetcher import RVLNewsAnalyzer



from utils.newsfilter_api import NewsfilterAPI


class SymbolMerger:
    def __init__(self, all_keys):
        self.all_keys = all_keys
        self.ny_today = datetime.now(ZoneInfo("America/New_York")).strftime('%Y-%m-%d')
        self.ny_tz = ZoneInfo("America/New_York")
        
    def merge(self, list_of_symbols, fundamentals, price_results):
        # Create mappings from symbols to their respective data
        symbol_map_fundamentals = {item['symbol']: item for item in fundamentals if item.get('symbol')}
        symbol_map_prices = {item['symbol']: item for item in price_results if item.get('symbol')}

        merged_results = []
        for symbol in list_of_symbols:
            record = {key: None for key in self.all_keys}
            record['symbol'] = symbol

            if symbol in symbol_map_fundamentals:
                record.update(symbol_map_fundamentals[symbol])
            if symbol in symbol_map_prices:
                record.update(symbol_map_prices[symbol])

            merged_results.append(record)

        return merged_results


class DataHandler:
    """
    èª¿è©¦ç‰ˆæœ¬çš„ DataHandler - æ·»åŠ è©³ç´°æ—¥èªŒä»¥æ‰¾å‡ºä¿å­˜å•é¡Œ
    """
    
    def __init__(self):
        self.polygon_controller = PolygonController()
        self.mongo_handler = MongoHandler()
        self.mongo_handler.create_collection('fundamentals_of_top_list_symbols')
        self.squeeze_scanner = ShortSqueezeScanner()
        
        # æ•¸æ“šå­˜å„²å±¬æ€§
        self.fundamentals = []
        self.list_of_symbols = []
        self._db_cache = {}
        self.ny_today = datetime.now(ZoneInfo("America/New_York")).strftime('%Y-%m-%d')
        self.ny_tz = ZoneInfo("America/New_York")

    def _get_db_documents(self, symbols=None, force_refresh=False):
        """çµ±ä¸€çš„æ•¸æ“šåº«æ–‡æª”ç²å–æ–¹æ³•ï¼Œå¸¶ç·©å­˜æ©Ÿåˆ¶"""
        if symbols is None:
            symbols = self.list_of_symbols
            
        cache_key = f"{'-'.join(sorted(symbols))}_{self.ny_today}"
        
        if not force_refresh and cache_key in self._db_cache:
            logger.info(f"Using cached data for {len(symbols)} symbols")
            return self._db_cache[cache_key]
        
        logger.info(f"Querying database for {len(symbols)} symbols on {self.ny_today}")
        documents = self.mongo_handler.find_doc(
            "fundamentals_of_top_list_symbols",
            {
                "symbol": {"$in": symbols},
                "today_date": self.ny_today
            }
        )
        
        logger.info(f"Found {len(documents)} documents in database")
        self._db_cache[cache_key] = documents
        return documents

    def check_merge_errors(self):
        """æª¢æŸ¥åˆä½µéŒ¯èª¤"""
        error_data = self.mongo_handler.find_doc(
            "fundamentals_of_top_list_symbols",
            {"close_change_percentage": {"$exists": False}}
        )
        print(f"Length of error data: {len(error_data)}")
        if len(error_data) > 0:
            logger.warning(f"Error: å·²æ‰¾åˆ° {len(error_data)} å€‹éŒ¯èª¤: Symbols: {error_data[0]['symbol']}")
            logger.warning(f"Error in fundamental data for symbol: {error_data[0]['symbol']}")
            
            # åˆªé™¤éŒ¯èª¤æ•¸æ“š
            for error_doc in error_data:
                try:
                    self.mongo_handler.delete_doc(
                        "fundamentals_of_top_list_symbols",
                        {"close_change_percentage": {"$exists": False}}
                    )
                    logger.info(f"å·²åˆªé™¤éŒ¯èª¤æ•¸æ“š: {error_doc['symbol']}")
                except Exception as e:
                    logger.error(f"åˆªé™¤éŒ¯èª¤æ•¸æ“šæ™‚å‡ºéŒ¯ {error_doc['symbol']}: {e}")
                    exit()
            
            logger.info(f"å·²åˆªé™¤æ‰€æœ‰éŒ¯èª¤æ•¸æ“š")
        else:
            logger.info(f"No errors in fundamental data")
        return False

    def get_fundamentals(self, symbol):
        """Get fundamental data for a single symbol using Polygon API."""
        try:
            details = self.polygon_controller.polygon_client.get_ticker_details(ticker=symbol)
            if not details or not hasattr(details, 'results'):
                return {'symbol': symbol}
            
            results = details.results
            return {
                'symbol': symbol,
                'name': results.get('name'),
                'listingExchange': results.get('primary_exchange'),
                'securityType': 'Common Stock' if results.get('type') == 'CS' else results.get('type'),
                'sector': results.get('sic_description'),
                'industry': results.get('sic_description'),
                'market': results.get('market'),
                'outstandingShares': {'$numberLong': str(results.get('share_class_shares_outstanding', 0))},
                'countryDomicile': 'US' if results.get('locale') == 'us' else results.get('locale'),
                'isin': None,
                'float': {'$numberLong': str(results.get('weighted_shares_outstanding', 0))},
                'annualDividend': None,
                'dividendFrequency': None,
                'beta': None,
                'bookValue': None,
                'earningsPerShare': None,
                'earningsPerShareTTM': None,
                'forwardEarningsPerShare': None,
                'turnoverPercentage': None,
                'averageVolume3M': None,
                'optionable': True,
                'lotSize': results.get('round_lot'),
                'lastSplitInfo': None,
                'lastSplitDate': None,
                'polygon_details': {
                    'active': results.get('active'),
                    'address': {
                        'address1': results.get('address', {}).get('address1'),
                        'city': results.get('address', {}).get('city'),
                        'postal_code': results.get('address', {}).get('postal_code'),
                        'state': results.get('address', {}).get('state')
                    },
                    'branding': {
                        'icon_url': results.get('branding', {}).get('icon_url'),
                        'logo_url': results.get('branding', {}).get('logo_url')
                    },
                    'cik': results.get('cik'),
                    'composite_figi': results.get('composite_figi'),
                    'currency_name': results.get('currency_name'),
                    'description': results.get('description'),
                    'homepage_url': results.get('homepage_url'),
                    'list_date': results.get('list_date'),
                    'market_cap': results.get('market_cap'),
                    'phone_number': results.get('phone_number'),
                    'share_class_figi': results.get('share_class_figi'),
                    'sic_code': results.get('sic_code'),
                    'ticker_root': results.get('ticker_root'),
                    'total_employees': results.get('total_employees'),
                    'weighted_shares_outstanding': results.get('weighted_shares_outstanding')
                },
                'today_date': datetime.now(ZoneInfo("America/New_York")).strftime('%Y-%m-%d')
            }
        except Exception as e:
            logger.error(f"Error fetching fundamentals for {symbol}: {e}")
            return {'symbol': symbol}

    def get_list_of_fundamentals(self, list_of_symbols):
        """Get fundamental data for multiple symbols using Polygon API."""
        fundamentals = []
        for symbol in list_of_symbols:
            fundamental = self.get_fundamentals(symbol)
            fundamentals.append(fundamental)
        return fundamentals

    def get_analyzer_data(self, symbol):
        """Get market data for a single symbol using ChartAnalyzer."""
        try:
            # ä½¿ç”¨æ–°çš„ ChartAnalyzer ä¾†ç²å–å’Œåˆ†ææ•¸æ“š
            analyzer = ChartAnalyzer(symbol)
            result = analyzer.run()
            
            # å°‡åˆ†æçµæœè½‰æ›ç‚ºæ‰€éœ€çš„æ ¼å¼
            return {
                'symbol': symbol,
                'premarket_high': result['premarket_high'],
                'premarket_low': result['premarket_low'],
                'market_open_high': result['market_open_high'],
                'market_open_low': result['market_open_low'],
                'day_high': result['day_high'],
                'day_low': result['day_low'],
                'day_close': result['day_close'],
                'yesterday_close': result['yesterday_close'],
                'high_change_percentage': result['high_change_percentage'],
                'close_change_percentage': result['close_change_percentage'],
                'most_volume_high': result['most_volume_high'],
                'most_volume_low': result['most_volume_low'],
                'key_levels': result.get('key_levels', []),
                '1m_chart_data': result['1m_chart_data'],
                '5m_chart_data': result['5m_chart_data'],
                '1d_chart_data': result['1d_chart_data']
            }
            
        except Exception as e:
            logger.error(f"Error in get_analyzer_data for {symbol}: {e}")
            return {
                'symbol': symbol,
                'premarket_high': None,
                'premarket_low': None,
                'market_open_high': None,
                'market_open_low': None,
                'day_high': None,
                'day_low': None,
                'day_close': None,
                'yesterday_close': None,
                'high_change_percentage': None,
                'close_change_percentage': None,
                'most_volume_high': None,
                'most_volume_low': None,
                'key_levels': [],
                '1m_chart_data': [],
                '5m_chart_data': [],
                '1d_chart_data': []
            }

    def fmt(self, value):
        """Format numeric values."""
        return round(value, 2) if isinstance(value, (int, float)) else value

    def get_price_analyzer_results(self, list_of_symbols):
        """Get price analysis results for multiple symbols."""
        price_analyzer_results = []
        for symbol in list_of_symbols:
            result = self.get_analyzer_data(symbol)
            price_analyzer_results.append(result)
            #print(result['1m_chart_data'][-1]['datetime'])
            #   print(result['5m_chart_data'][-1]['datetime'])
            #print(result['1d_chart_data'][-1]['datetime'])
        return price_analyzer_results

    def merge_fundamentals_and_price_data(self, list_of_symbols, fundamentals, price_analyzer_results):
        """Merge fundamental data with price analysis data."""
        # Define all possible fields
        all_keys = [
            'symbol', 'name', 'listingExchange', 'securityType', 'countryDomicile', 'countryIncorporation',
            'isin', 'sector', 'industry', 'lastSplitInfo', 'lastSplitDate', 'lotSize', 'optionable',
            'earningsPerShare', 'earningsPerShareTTM', 'forwardEarningsPerShare', 'nextEarnings',
            'annualDividend', 'last12MonthDividend', 'lastDividend', 'exDividend', 'dividendFrequency',
            'beta', 'averageVolume3M', 'turnoverPercentage', 'bookValue', 'sales', 'outstandingShares', 'float',
            # Add price fields
            'premarket_high', 'premarket_low', 'market_open_high', 'market_open_low', 'day_high', 'day_low',
            'day_close', 'yesterday_close', 'high_change_percentage', 'close_change_percentage',
            'most_volume_high', 'most_volume_low'
        ]

        merger = SymbolMerger(all_keys)
        return merger.merge(list_of_symbols, fundamentals, price_analyzer_results)

    def perform_short_squeeze_analysis(self):
        """Perform short squeeze analysis on fundamental data."""
        logger.info("Starting Short Squeeze Analysis")
        list_of_short_squeeze_results = []
        
        for fundamental in self.fundamentals:
            short_squeeze_results = self.squeeze_scanner.run(
                new_stock_data=fundamental,
                current_price=fundamental['day_close'],
                intraday_high=fundamental['day_high'],
                short_interest=None,
                as_json=True
            )
            list_of_short_squeeze_results.append(short_squeeze_results)
            logger.info(f"Short Squeeze Analysis for {fundamental['symbol']} Ready!")
            #self.squeeze_scanner.print_readable_analysis()

        logger.info(f"Short Squeeze Analysis Lengths: {len(list_of_short_squeeze_results)}")
        
        # å„ªåŒ–çš„åˆä½µæ–¹æ³•ï¼šç›´æ¥å­—å…¸æ›´æ–°
        squeeze_results_map = {item['symbol']: item for item in list_of_short_squeeze_results if item.get('symbol')}
        
        for fundamental_item in self.fundamentals:
            symbol = fundamental_item.get('symbol')
            if symbol and symbol in squeeze_results_map:
                # ç›´æ¥æ›´æ–°ç¾æœ‰çš„ fundamental å­—å…¸ï¼Œé¿å…è¦†è“‹é‡è¦å­—æ®µ
                squeeze_data = squeeze_results_map[symbol].copy()
                squeeze_data.pop('symbol', None)  # ç§»é™¤ symbol éµé¿å…é‡è¤‡
                fundamental_item.update(squeeze_data)
                logger.info(f"Short Squeeze Analysis Merged into Fundamentals for {symbol} !")
        
        logger.info(f"Successfully merged short squeeze analysis for {len(squeeze_results_map)} symbols")
        return self.fundamentals

    def handle_symbols(self, list_of_symbols):
        """Process symbols to get fundamentals, price data, and short squeeze analysis."""
        logger.info(f"è™•ç† {len(list_of_symbols)} å€‹ç¬¦è™Ÿçš„æ•¸æ“š")
        
        # Get price analysis results
        price_analyzer_results = self.get_price_analyzer_results(list_of_symbols)
        logger.info(f"ç²å–åˆ° {len(price_analyzer_results)} å€‹åƒ¹æ ¼åˆ†æçµæœ")
        
        # Get fundamental data
        fundamentals = self.get_list_of_fundamentals(list_of_symbols)
        logger.info(f"ç²å–åˆ° {len(fundamentals)} å€‹åŸºæœ¬é¢æ•¸æ“š")
        
        # Merge fundamental and price data
        self.fundamentals = self.merge_fundamentals_and_price_data(list_of_symbols, fundamentals, price_analyzer_results)
        logger.info(f"åˆä½µå¾Œçš„åŸºæœ¬é¢æ•¸æ“šé•·åº¦: {len(self.fundamentals)}")
        
        # æª¢æŸ¥åˆä½µå¾Œæ•¸æ“šçš„çµæ§‹
        if self.fundamentals:
            sample_keys = list(self.fundamentals[0].keys())
            logger.info(f"æ¨£æœ¬æ•¸æ“šå­—æ®µ: {sample_keys[:10]}...")  # åªé¡¯ç¤ºå‰10å€‹å­—æ®µ
        
        # Perform short squeeze analysis
        return self.perform_short_squeeze_analysis() #return fundamentals with short squeeze analysis

    def store_fundamentals_in_db(self):
        """Store or update fundamental data in MongoDB - æ·»åŠ è©³ç´°èª¿è©¦ä¿¡æ¯"""
        logger.info(f"é–‹å§‹ä¿å­˜ {len(self.fundamentals)} å€‹åŸºæœ¬é¢æ•¸æ“šåˆ°æ•¸æ“šåº«")
        
        # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
        for i, fundamental in enumerate(self.fundamentals):
            if not fundamental.get('symbol'):
                logger.error(f"ç¬¬ {i} å€‹åŸºæœ¬é¢æ•¸æ“šç¼ºå°‘ symbol å­—æ®µ: {fundamental}")
                continue
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„åƒ¹æ ¼æ•¸æ“š
            required_fields = ['day_close', 'close_change_percentage']
            missing_fields = [field for field in required_fields if field not in fundamental or fundamental[field] is None]
            if missing_fields:
                logger.warning(f"Symbol {fundamental['symbol']} ç¼ºå°‘å­—æ®µ: {missing_fields}")
        
        ny_time = datetime.now(ZoneInfo("America/New_York"))
        ny_today = ny_time.strftime('%Y-%m-%d')
        date_list = [(ny_time - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(7)]
        
        logger.info(f"æŸ¥è©¢æœ€è¿‘ 7 å¤©çš„æ•¸æ“š: {date_list}")

        # Query recent fundamental documents
        recent_fundamental_docs = self.mongo_handler.find_doc(
            "fundamentals_of_top_list_symbols",
            {
                "symbol": {"$in": [f["symbol"] for f in self.fundamentals]},
                "today_date": {"$in": date_list}
            }
        )
        
        logger.info(f"æ‰¾åˆ° {len(recent_fundamental_docs)} å€‹æœ€è¿‘çš„åŸºæœ¬é¢æ–‡æª”")
        recent_symbols = set(doc["symbol"] for doc in recent_fundamental_docs)
        
        # ä¿å­˜è¨ˆæ•¸å™¨
        saved_count = 0
        updated_count = 0

        for fundamental in self.fundamentals:
            symbol = fundamental["symbol"]
            fundamental["today_date"] = ny_today

            # å–å‡ºé€™å€‹ symbol æœ€è¿‘ 7 å¤©çš„èˆŠè³‡æ–™
            recent_docs = [doc for doc in recent_fundamental_docs if doc["symbol"] == symbol]
            
            if recent_docs:
                # æ‰¾åˆ°æœ€æ–°çš„é‚£ç­†ï¼ˆè‹¥æœ‰å¤šç­†ï¼‰
                latest_doc = max(recent_docs, key=lambda d: d["today_date"])

                # ä¿ç•™èˆŠè³‡æ–™ä¸­å…¶ä»–å­—æ®µï¼Œæ›´æ–°ç‚ºæ–°è³‡æ–™ï¼ˆæ–°è³‡æ–™å„ªå…ˆï¼‰
                merged_data = {**latest_doc, **fundamental}
                merged_data["today_date"] = ny_today  # ä¸€å®šæ˜¯ä»Šå¤©

                try:
                    result = self.mongo_handler.upsert_doc(
                        "fundamentals_of_top_list_symbols",
                        {"symbol": symbol, "today_date": ny_today},
                        merged_data
                    )
                    logger.info(f"æ›´æ–° {symbol}ï¼šåˆä½µèˆŠè³‡æ–™å¾Œä¿å­˜ç‚ºä»Šå¤©çš„è³‡æ–™")
                    updated_count += 1
                except Exception as e:
                    logger.error(f"æ›´æ–° {symbol} æ™‚å‡ºéŒ¯: {e}")
            else:
                # æ²’æœ‰èˆŠè³‡æ–™ï¼Œç›´æ¥æ’å…¥ä»Šå¤©çš„è³‡æ–™
                try:
                    result = self.mongo_handler.upsert_doc(
                        "fundamentals_of_top_list_symbols",
                        {"symbol": symbol, "today_date": ny_today},
                        fundamental
                    )
                    logger.info(f"æ–°å¢ {symbol} ç‚ºä»Šå¤©çš„æ–°è³‡æ–™")
                    saved_count += 1
                except Exception as e:
                    logger.error(f"å„²å­˜ {symbol} æ™‚å‡ºéŒ¯: {e}")

        
        # é©—è­‰ä¿å­˜çµæœ
        time.sleep(1)
        verification_docs = self.mongo_handler.find_doc(
            "fundamentals_of_top_list_symbols",
            {
                "symbol": {"$in": [f["symbol"] for f in self.fundamentals]},
                "today_date": ny_today
            }
        )
        logger.info(f"é©—è­‰: æ•¸æ“šåº«ä¸­æ‰¾åˆ° {len(verification_docs)} å€‹ä»Šæ—¥æ–‡æª”")
        
        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰æ•¸æ“šéƒ½å·²ä¿å­˜
        saved_symbols = set(doc["symbol"] for doc in verification_docs)
        missing_symbols = set(f["symbol"] for f in self.fundamentals) - saved_symbols
        if missing_symbols:
            logger.error(f"ä»¥ä¸‹ç¬¦è™Ÿçš„æ•¸æ“šæœªèƒ½ä¿å­˜åˆ°æ•¸æ“šåº«: {missing_symbols}")
        else:
            logger.info("æ‰€æœ‰åŸºæœ¬é¢æ•¸æ“šå·²æˆåŠŸä¿å­˜åˆ°æ•¸æ“šåº«")

    def process_suggestions(self):
        """çµ±ä¸€è™•ç†å»ºè­°çš„æ–¹æ³• - ä½¿ç”¨ Polygon API çš„æ–°èæ•¸æ“šå’Œ OpenAI åˆ†æ"""
        logger.info("Processing suggestions...")
        
        # ç²å–æ•¸æ“šåº«ä¸­å·²æœ‰å»ºè­°çš„æ–‡æª”
        documents = self._get_db_documents()
        existing_suggestions_symbols = {
            doc["symbol"] for doc in documents 
            if doc.get("suggestion")
        }
        
        logger.info(f"æ‰¾åˆ° {len(existing_suggestions_symbols)} å€‹å·²æœ‰å»ºè­°çš„ç¬¦è™Ÿ")
        
        # æ‰¾å‡ºéœ€è¦åˆ†æçš„æ–°ç¬¦è™Ÿ
        symbols_to_analyze = [
            symbol for symbol in self.list_of_symbols 
            if symbol not in existing_suggestions_symbols
        ]
        
        # ç‚ºæ–°ç¬¦è™Ÿç²å–æ–°èå’Œåˆ†æ
        if symbols_to_analyze:
            logger.info(f"æ­£åœ¨ç‚º {len(symbols_to_analyze)} å€‹æ–°ç¬¦è™Ÿåˆ†æå»ºè­°")
            new_suggestions = []
            
            # åˆå§‹åŒ– Summarizer (åŒ…å« OpenAI å®¢æˆ¶ç«¯)
            summarizer = Summarizer()
            
            for symbol in symbols_to_analyze:
                print("================================================")
                print(f"\n\n\nğŸ“° Fetching news...:{symbol}")
                
                news_data = []  # Initialize news_data list here
                
                try:
                    # ä½¿ç”¨ Polygon API ç²å–æ–°è
                    news_generator = self.polygon_controller.polygon_client.list_ticker_news(
                        ticker=symbol,
                        limit=5,
                        order='desc',
                        sort='published_utc'
                    )
                    
                    # å°‡ç”Ÿæˆå™¨è½‰æ›ç‚ºåˆ—è¡¨
                    news_list = list(news_generator) if news_generator else []
                    
                    if news_list:
                        # è™•ç†æ–°èæ•¸æ“š
                        for item in news_list:
                            try:
                                # è½‰æ›æ™‚é–“æˆ³ç‚ºUTCæ™‚é–“
                                published_time = datetime.fromisoformat(item.published_utc.replace('Z', '+00:00'))
                                # ç¢ºä¿æ™‚é–“æ˜¯UTCæ™‚å€
                                if published_time.tzinfo is None:
                                    published_time = published_time.replace(tzinfo=ZoneInfo("UTC"))
                                utc_timestamp = int(published_time.timestamp())
                                
                                news_item = {
                                    "title": item.title,
                                    "link": item.article_url,
                                    "publisher": item.publisher.name if hasattr(item.publisher, 'name') else item.publisher,
                                    "symbols": [symbol.upper()],  # ä¿æŒèˆ‡åŸæ ¼å¼ä¸€è‡´
                                    "utcTime": utc_timestamp,
                                    "keywords": item.tickers if hasattr(item, 'tickers') else []
                                }
                                news_data.append(news_item)
                            except Exception as e:
                                logger.error(f"è™•ç†æ–°èé …ç›®æ™‚å‡ºéŒ¯: {e}")
                                continue

                        
                    # region Newsfilter API
                    news_filter_api = NewsfilterAPI()
                    news_filter_api_result = news_filter_api.get_news_from_newsfilter(symbol)
                    if news_filter_api_result and news_filter_api_result.get('articles'):
                        for article in news_filter_api_result['articles']:
                            try:
                                # Convert publishedAt string to UTC timestamp
                                published_time = datetime.fromisoformat(article['publishedAt'].replace('Z', '+00:00'))
                                if published_time.tzinfo is None:
                                    published_time = published_time.replace(tzinfo=ZoneInfo("UTC"))
                                utc_timestamp = int(published_time.timestamp())
                                
                                news_item = {
                                    "title": article['title'],
                                    "link": article['url'],
                                    "publisher": article['source']['name'],
                                    "symbols": [symbol.upper()],
                                    "utcTime": utc_timestamp,
                                    "keywords": [],
                                    "html_content": article.get('html_content', '')  # Use get() with default value
                                }
                                print(f"ğŸ” ===========News Item: {news_item}")
                                news_data.append(news_item)
                            except Exception as e:
                                logger.error(f"è™•ç† Newsfilter æ–°èé …ç›®æ™‚å‡ºéŒ¯: {e}")
                                continue

                    if news_data:
                        # ä½¿ç”¨åŸæœ‰çš„ RVLNewsAnalyzer ä¾†è™•ç†æ–°è
                        news_analyzer = RVLNewsAnalyzer()
                        summaries = news_analyzer.analyze(news_data)
                        print(f"\n\n ğŸ“° Summaries for {symbol.upper()}: {json.dumps(summaries, indent=2, ensure_ascii=False)}")
                        
                        if summaries != []:
                            suggestion = summarizer.suggestion(summaries)
                            print(f"\n\n ğŸ“° Suggestion for {symbol.upper()}: {suggestion}")
                        else:
                            suggestion = "No recent news available"
                        
                        new_suggestions.append({"symbol": symbol, "suggestion": suggestion})
                        
                        # ä¿å­˜åˆ°æ•¸æ“šåº«
                        try:
                            result = self.mongo_handler.upsert_doc(
                                "fundamentals_of_top_list_symbols",
                                {"symbol": symbol, "today_date": datetime.now(ZoneInfo("UTC")).strftime('%Y-%m-%d')},
                                {"suggestion": suggestion}
                            )
                            logger.info(f"ä¿å­˜å»ºè­° {symbol}: {result}")
                        except Exception as e:
                            logger.error(f"ä¿å­˜å»ºè­° {symbol} æ™‚å‡ºéŒ¯: {e}")
                    
                    else:
                        logger.warning(f"æ²’æœ‰æ‰¾åˆ° {symbol} çš„æ–°è")
                        new_suggestions.append({"symbol": symbol, "suggestion": "No recent news available"})
                        
                except Exception as e:
                    logger.error(f"âŒ Failed to fetch news for {symbol.upper()}: {e}")
                    if hasattr(e, "response") and e.response:
                        logger.error(f"Status Code: {e.response.status_code}")
                        logger.error(f"Response: {e.response.text}")
                    new_suggestions.append({"symbol": symbol, "suggestion": f"Error fetching news: {str(e)}"})

            # åˆ·æ–°ç·©å­˜
            self._get_db_documents(force_refresh=True)
            
            # æ‰“å°æ–°å»ºè­°
            self.print_readable_suggestions(new_suggestions)
            
            return len(new_suggestions)
        
        logger.info("No new symbols need suggestion analysis")
        return 0

    def process_sec_analysis(self):
        """çµ±ä¸€è™•ç†SECåˆ†æçš„æ–¹æ³•"""
        logger.info("Processing SEC filing analysis...")
        
        # ç²å–å·²æœ‰SECåˆ†æçš„ç¬¦è™Ÿ
        documents = self._get_db_documents()
        analyzed_symbols = {
            doc["symbol"] for doc in documents 
            if doc.get("sec_filing_analysis")
        }
        
        logger.info(f"æ‰¾åˆ° {len(analyzed_symbols)} å€‹å·²æœ‰SECåˆ†æçš„ç¬¦è™Ÿ")
        
        # æ‰¾å‡ºéœ€è¦åˆ†æçš„ç¬¦è™Ÿ
        symbols_to_analyze = [
            symbol for symbol in self.list_of_symbols 
            if symbol not in analyzed_symbols
        ]
        
        if symbols_to_analyze:
            logger.info(f"æ­£åœ¨ç‚º {len(symbols_to_analyze)} å€‹ç¬¦è™Ÿåˆ†æSECæ–‡ä»¶")
            analyzer = SECFinancialAnalyzer()
            analyzer.SYMBOL_LIST = symbols_to_analyze
            analysis_results = analyzer.run_analysis()

            # ç›´æ¥æ›´æ–°åˆ°æ•¸æ“šåº«
            for analysis_result in analysis_results:
                symbol = analysis_result["Symbol"]
                try:
                    result = self.mongo_handler.upsert_doc(
                        "fundamentals_of_top_list_symbols",
                        {"symbol": symbol, "today_date": self.ny_today},
                        {"sec_filing_analysis": analysis_result}
                    )
                    logger.info(f"ä¿å­˜SECåˆ†æ {symbol}: {result}")
                except Exception as e:
                    logger.error(f"ä¿å­˜SECåˆ†æ {symbol} æ™‚å‡ºéŒ¯: {e}")
            
            # åˆ·æ–°ç·©å­˜
            self._get_db_documents(force_refresh=True)
            
            return len(analysis_results)
        
        logger.info("No new symbols need SEC analysis")
        return 0

    def build_final_results(self):
        """æ§‹å»ºæœ€çµ‚çµæœ"""
        logger.info("Building final results...")
        
        # ç²å–æ‰€æœ‰ä»Šæ—¥çš„åŸºæœ¬é¢æ–‡æª”
        documents = self._get_db_documents()
        
        # æ§‹å»ºæœ€çµ‚çµæœ
        final_fundamentals = []
        final_sec_analyses = []
        
        for doc in documents:
            # åŸºæœ¬é¢æ•¸æ“š
            final_fundamentals.append(doc)
            
            # SECåˆ†æçµæœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if "sec_filing_analysis" in doc:
                final_sec_analyses.append(doc["sec_filing_analysis"])
        
        logger.info(f"Final results: {len(final_fundamentals)} fundamentals, {len(final_sec_analyses)} SEC analyses")
        
        return final_fundamentals, final_sec_analyses

    def print_readable_suggestions(self, suggestions: list[dict]):
        """Print suggestions in a human-readable format."""
        for item in suggestions:
            symbol = item.get("symbol", "Unknown symbol")
            suggestion = item.get("suggestion", "(No suggestion content)")

            print("====================================")
            print(f"ğŸ“Œ Suggestion for {symbol}:\n\n")
            print(suggestion)
            print("\n")

    def get_positions(self):
        """Placeholder for retrieving current positions from trading account."""
        pass

    def get_accounts(self):
        """Placeholder for retrieving account information."""
        pass

    def run(self, list_of_symbols):
        """
        èª¿è©¦ç‰ˆæœ¬çš„ä¸»åŸ·è¡Œæ–¹æ³•
        """
        logger.info(f"=== é–‹å§‹é‹è¡Œ DataHandlerï¼Œè™•ç† {len(list_of_symbols)} å€‹ç¬¦è™Ÿ ===")
        logger.info(f"ç¬¦è™Ÿåˆ—è¡¨: {list_of_symbols}")
        
        self.list_of_symbols = list_of_symbols
        self._db_cache.clear()  # æ¸…ç©ºç·©å­˜
        
        # Step 1: è™•ç†ç¬¦è™Ÿä¸¦ç²å–åŸºæœ¬é¢æ•¸æ“šå’Œåˆ†æ
        logger.info("=== Step 1: è™•ç†ç¬¦è™Ÿå’ŒåŸºæœ¬é¢æ•¸æ“š ===")
        self.fundamentals = self.handle_symbols(self.list_of_symbols) #return fundamentals with short squeeze analysis
        
        if not self.fundamentals:
            logger.error("âŒ æ²’æœ‰ç²å–åˆ°ä»»ä½•åŸºæœ¬é¢æ•¸æ“šï¼")
            return [], []
        
        logger.info(f"âœ… æˆåŠŸè™•ç† {len(self.fundamentals)} å€‹åŸºæœ¬é¢æ•¸æ“š")
        
        # Step 2: å°‡åŸºæœ¬é¢æ•¸æ“šå­˜å„²åˆ°æ•¸æ“šåº«
        logger.info("=== Step 2: å­˜å„²åŸºæœ¬é¢æ•¸æ“šåˆ°æ•¸æ“šåº« ===")
        self.store_fundamentals_in_db()
        
        # Step 3: è™•ç†å»ºè­°
        logger.info("=== Step 3: è™•ç†å»ºè­° ===")
        new_suggestions_count = self.process_suggestions()
        
        # Step 4: è™•ç†SECåˆ†æ
        logger.info("=== Step 4: è™•ç†SECåˆ†æ ===")
        new_analyses_count = self.process_sec_analysis()
        
        # Step 5: æ§‹å»ºæœ€çµ‚çµæœ
        logger.info("=== Step 5: æ§‹å»ºæœ€çµ‚çµæœ ===")
        final_fundamentals, final_sec_analyses = self.build_final_results()
        
        # Step 6: æª¢æŸ¥åˆä½µéŒ¯èª¤
        logger.info("=== Step 6: æª¢æŸ¥åˆä½µéŒ¯èª¤ ===")
        self.check_merge_errors()
        
        logger.info(f"""
        === è™•ç†å®Œæˆï¼ ===
        - è™•ç†ç¬¦è™Ÿæ•¸é‡: {len(self.list_of_symbols)}
        - æ–°å»ºè­°æ•¸é‡: {new_suggestions_count}
        - æ–°SECåˆ†ææ•¸é‡: {new_analyses_count}
        - è¿”å›åŸºæœ¬é¢æ•¸æ“š: {len(final_fundamentals)}
        - è¿”å›SECåˆ†æ: {len(final_sec_analyses)}
        """)
        
        return final_fundamentals, final_sec_analyses

class Summarizer:
    def __init__(self):
        self.key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=self.key)

    def summarize(self, text: str) -> str:
        prompt = f"è«‹æ ¹æ“šä»¥ä¸‹æ–°èå…§å®¹ç”Ÿæˆç¹é«”ä¸­æ–‡ç°¡çŸ­æ‘˜è¦ï¼š\n\n{str(text)}"
        completion = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "developer", "content": "You are a helpful assistant. You take news content as input and generate a detailed summary. Do not include the original news content in the summary."},
                {"role": "user", "content": prompt}
            ]
        )
        summary = completion.choices[0].message.content.strip()
        print(f"\nğŸ“° Summary: {summary}\n")
        return summary

    def suggestion(self, text: str) -> str:
        prompt = f"è«‹æ ¹æ“šä»¥ä¸‹æ–°èå…§å®¹ç”Ÿæˆç°¡çŸ­å»ºè­°ï¼š\n\n{str(text)}"
        completion = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "developer", "content": "ç”¨æˆ¶æ˜¯ä¸€å€‹æ—¥å…§äº¤æ˜“è€…, ä»–ä¸»è¦çš„æ˜¯åšç©ºè‚¡ç¥¨çš„äº¤æ˜“è€…, ç”¨æˆ¶æœƒæä¾›æœ€æ–°çš„æ–°èçš„ä¸€äº›ç¸½çµ, å¦‚æœæ–°èä¸­æœ‰éå¸¸å¼·çš„æ­£é¢æƒ…ç·’, è«‹å‘ç”¨æˆ¶åˆ—å‡ºé¢¨éšª, è§£é‡‹ç‚ºä½•ä¸å»ºè­°åšç©º, ä½†å¦‚æœæ²’æœ‰ç•¶å¤©çš„æ–°è, æˆ–æ–°èä¸­çš„æ­£é¢æƒ…ç·’ä¸é«˜, è«‹å‘ç”¨æˆ¶åˆ—å‡ºå»ºè­°ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        summary = completion.choices[0].message.content.strip()
        print(f"\nğŸ“° Summary: {summary}\n")
        return summary